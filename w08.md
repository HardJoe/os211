---
layout: "layout"
permalink: /W08/
---

# Top 10 List of Week 08

1. [What is Scheduling?](https://www.javatpoint.com/os-cpu-scheduling)<br>
When there are multiple processes in OS, we need to find a schedule for arranging the sequence of the processes to be done.
The OS has to define which process the CPU that will be given.
OS has a process control block to control all of the processes.
This article will tell you about why what is saved in the process control block and why we need scheduling.

2. [What is meant by CPU Burst and I/O Burst?](https://youtu.be/SsiRHvFUpQQ)<br>
So we keep hearing about burst in scheduling.
What is the point of these bursts anyway?
You can find the answer in this video.
A burst is like a brief stretch of run as fast as you can go until you cant. 
A CPU burst occurs when it is executing instructions, meanwhile an I/O system burst occurs when it services requests to fetch information.

3. [Scheduling Criteria](https://youtu.be/eoGUQ5sl0vw)<br>
Different CPU scheduling algorithms have different properties and the choice of a particular algorithm depends on the various factors.
With this, there are lots of criteria for comparing CPU scheduling algorithms.
Some of them are throughput, turnaround time, waiting time, response time, etc.
Check this video to see the details of these criteria.

4. [Scheduling Algorithms in OS?](https://www.w3schools.in/operating-system-tutorial/scheduling-algorithms/)<br>
Some of the schedulings in OS require different algorithms. looking at only a single CPU burst each for a small number of processes. Obviously real systems have to deal with a lot more simultaneous processes executing their CPU-I/O burst cycles.
We have the First-Come First-Served, shortest-job-first, priority, round-robin, and multilevel queue.
Each has its own strengths and weaknesses.
Wanna see the implementations and the best conditions to use these algorithms?
It's provided on this site!

5. [Difference between preemptive and non-preemptive scheduling?](https://afteracademy.com/blog/what-is-the-difference-between-preemptive-and-non-preemptive-scheduling)<br>
The algorithms explained in point (5) can be divided into two categories, preemptive and non-preemptive.
In preemptive scheduling, the CPU will execute a process but for a limited period of time and after that, the process has to wait for its next turn.
However in non-preemptive scheduling, if some resource is allocated to a process then that resource will not be taken back until the completion of the process.
Check this site to know which algorithms belong to each of the categories.

6. [Linux Completely Fair Scheduler](https://opensource.com/article/19/2/fair-scheduling-linux)<br>
Most modern operating systems are designed to try to extract optimal performance from underlying hardware resources. 
Like most modern operating systems, Linux is a multitasking operating system, and therefore, it has a scheduler.
It's called Completely Fair Scheduler (CFS).
In contrast to the previous O(1) scheduler used in older Linux 2.6 kernels, the CFS scheduler implementation is not based on run queues, instead, it uses a redblack tree.
Check this site to see the details of CFS.

7. [AMP vs SMP](https://www.embedded.com/multicore-basics-amp-and-smp/)<br>
AMP stands for Asymmetric Multi-Processing and SMP means Symmetric Multi-Processing.
An AMP system has multiple CPUs, each of which may be a different architecture (but can be the same. AMP is most likely to be used when different CPU architectures are optimal for specific activities.
Like AMP, an SMP system has multiple CPUs, but in this case each has exactly the same architecture.
Typically, SMP is used when an embedded application simply needs more CPU power to manage its workload, in much the way that multicore CPUs are used in desktop computers.
See this site to see the specific differences between AMP and SMP.

8. [Understanding NUMA Architecture](https://linuxhint.com/understanding_numa_architecture/)<br>
NUMA is a computing system composed of several single nodes in such a way that the aggregate memory is shared between all nodes.
NUMA is a clever system used for connecting multiple central processing units (CPU) to any amount of computer memory available on the computer.
This reading is interesting because it explains to you about the evolution of shared memory multiprocessors, software support for NUMA, and its usage scenario.

9. [How does a multi-core processor work?](https://www.geekboots.com/story/how-does-multi-core-processor-work)<br>
A multi-core processor is one which combines two or more independent processors into a single package, often in a single integrated circuit to perform task parallel, increasing the performance of software which is written to take advantage of the unique architecture.
The first multicore processors were produced by Intel and AMD in the early 2000s.
See how a multi-core processor works here!

10. [Two State Process Model](https://t4tutorials.com/two-state-process-model-in-operating-systems/)<br>
When a process executes, it goes through a number of states. The current state of the process tells us about the current activity of the process. The state of a process may change due to events like I/O requests, interrupt routines, synchronization of processes, process scheduling algorithms, etc.
See this article to learn how to make a two state process model.
